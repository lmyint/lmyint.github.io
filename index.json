[{"authors":null,"categories":null,"content":"I am an Assistant Professor of Statistics at Macalester College in the Department of Mathematics, Statistics, and Computer Science. I love using statistics to learn about biology and human health. Methodologically, I have a particular passion for looking at data from high-throughput technologies and using causal inference and graph-based tools. Besides health and biology, I also have interests in human-data interaction: the investigation of how humans interact with data in ways that impact analysis, understanding, and scientific communication. I am also quite excited about creating tools that help people organize and explore large amounts of interconnected information.\nI received my PhD in biostatistics from the Johns Hopkins Bloomberg School of Public Health where I worked with Kasper Daniel Hansen on statistical methodology for high-throughput biology and with Jeff Leek and Leah Jager on understanding the role of human behavior in data analysis.\nMy last name is pronounced mee-int. A common mispronunciation that is quite understandable given the spelling is my-int. While this can be amusing from a computer science perspective, it is, nevertheless, incorrect!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an Assistant Professor of Statistics at Macalester College in the Department of Mathematics, Statistics, and Computer Science. I love using statistics to learn about biology and human health. Methodologically, I have a particular passion for looking at data from high-throughput technologies and using causal inference and graph-based tools.","tags":null,"title":"Leslie Myint","type":"authors"},{"authors":null,"categories":["R"],"content":"I love Dungeons and Dragons. I am also a data-loving statistician. At some point, these worlds were bound to collide.\nFor those unfamiliar with Dungeons and Dragons (DnD), it is a role-playing game that is backed by an extraodinary amount of data. The overall gist is that players create characters that band together with other characters to travel the world and adventure. Essentially, it\u0026rsquo;s collective storytelling aided by dice as vehicles of chance and uncertainty. Where does data come in? Through the world-building content that is released by the official creators and by players. This content helps players build characters that have a range of characteristics and abilities governed by their past and occupation. This content similarly helps shape the monsters and enemies that the characters may face.\nThere is a wonderful digital resource for DnD content called DnD Beyond that contains information on characters, monsters, and treasures. (No API yet, but it is apparently in the works.) For a while, I\u0026rsquo;ve been interested in playing around with data on monster statistics, and I finally got around to it this week! I had been reluctant to start because I did not have a clear idea of how to scrape pages that required login via redirect to an external authentication service (here, Twitch). I\u0026rsquo;ll go over the specific hurdles and solutions in this post. I\u0026rsquo;ll also give a general tutorial for scraping with rvest.\nAll of the code for this post is available at https://github.com/lmyint/dnd_analysis.\nTable of Contents  Structure of the scraping task Step 1: Scrape tables to get individual monster page URLs  General structure of rvest code SelectorGadget Extract URLs   Step 2: Use RSelenium to access pages behind login  What did not work What did work  Step 2a: Start automated browsing with rsDriver Step 2b: Browser navigation and interaction Step 2c: Extract page source     Step 3: Write a function to scrape an individual page Summary  Structure of the scraping task  If you go to https://www.dndbeyond.com/monsters, you will see the first of several tens of pages of monster listings. You will also see that each monster name is a link to an individual monster page that contains more extensive details about that monster\u0026rsquo;s statistics, abilities, and lore. An example that is free to view is the Adult Green Dragon. Other monsters that are not part of the Basic Rules set can only be viewed if you are signed in and have purchased the digital book in which that monster is contained.\nThe first part of the scraping task is to the scrape the several pages of tables starting at https://www.dndbeyond.com/monsters in order to get the links to individual monster pages.\nThe second part of the scraping task is to scrape the individual monster pages, such as the Adult Green Dragon.\nThroughout, I use the following packages:\n rvest for page scraping stringr for working with strings tibble for the flexibility over data frames to allow list-columns RSelenium for browser navigation via R. This package was on CRAN but removed in May 2018. I used the development version on GitHub, but the package maintainer is currently working to fix this.  Step 1: Scrape tables to get individual monster page URLs  By visiting a few different pages of monster results, we can see that the URLs for the page results have a consistent format: https://www.dndbeyond.com/monsters?page=NUM where NUM ranges from 1 to the last page. We can obtain the last page number programatically with the following:\npage \u0026lt;- read_html(\u0026quot;https://www.dndbeyond.com/monsters\u0026quot;) num_pages \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;.b-pagination-item\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% as.integer() %\u0026gt;% max(na.rm = TRUE)  Let\u0026rsquo;s explore the anatomy of this code to better understand how to work with rvest.\nGeneral structure of rvest code  The small example above shows the power of rvest. In many cases, the code to scrape content on a webpage really does boil down to something as short as:\nurl %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;CSS or XPATH selector\u0026quot;) %\u0026gt;% html_text() OR html_attr()    We start with a URL string that is passed to the read_html function. This creats an XML document object which is a tree representation of the content on a webpage. Why a tree? This requires some familiarity with HTML, but essentially text content is nested within enclosing formatting tags to make up a webpage. The following diagram from a W3Schools tutorial illustrates this.   The html_nodes function takes a string specifying the HTML tags that you desire to be selected. The selector string can be a CSS or XPATH selector. I only know about CSS selectors, and that has sufficed for all of my web scraping to date. This function returns a list of nodes that have been selected from the HTML tree. For example, selecting the \u0026lt;body\u0026gt; tag is like grabbing the trunk of the HTML tree, and selecting paragraph \u0026lt;p\u0026gt; tags is like grabbing only the thinner branches. This list of nodes is still a list of XML objects.\n  Usually what we want in scraping is the text that we see on the webpage that is contained within the specific sections extracted with html_nodes. We can get this text with html_text. Often we will also want attributes of the text on a webpage. For example, we may see text that is actually a link, and we want the URL for that link. In this case html_text would not give us what we want because it would give us the link text. However, html_attr will allow us to extract the URL. A specific example of this in just a second!\n  SelectorGadget  Back to the code example above:\npage \u0026lt;- read_html(\u0026quot;https://www.dndbeyond.com/monsters\u0026quot;) num_pages \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;.b-pagination-item\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% as.integer() %\u0026gt;% max(na.rm = TRUE)  The most difficult part of this part of code is figuring out the selector to use in html_nodes. Luckily, the rvest package page on CRAN has a link to a vignette on a tool called SelectorGadget. I love this tool for its playful homage to childhood memories, and it also greatly helps in determining the CSS selectors needed to select desired parts of a webpage. Once you have dragged the tool link to the bookmark bar, you can click the bookmark while viewing any page to get a hover tool that highlights page elements as you mouse over them. Clicking on an element on the page displays the text for the CSS selector in the tool panel.\nUsing the SelectorGadget tool, we can determine that the page number buttons on the main monster page all have the class b-pagination-item. The CSS selector for a class always starts with a period followed by the class name. The last page was the maximum of these numbers. (We need to remove NA\u0026rsquo;s created by integer coercion of the \u0026ldquo;Next\u0026rdquo; button.)\nExtract URLs  Now that we know how many pages (num_pages) to loop through, let\u0026rsquo;s write a function that will extract the URLs for the individual monster pages that are present on a single page of results.\nget_monster_links \u0026lt;- function(url) { page \u0026lt;- read_html(url) rel_links \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;.link\u0026quot;) %\u0026gt;% html_attr(name = \u0026quot;href\u0026quot;) keep \u0026lt;- str_detect(rel_links, \u0026quot;/monsters/\u0026quot;) rel_links \u0026lt;- rel_links[keep] abs_links \u0026lt;- paste0(\u0026quot;https://www.dndbeyond.com\u0026quot;, rel_links) abs_links }  The get_monster_links function takes as input a URL for a page of results (like https://www.dndbeyond.com/monsters?page=2). Let\u0026rsquo;s work through the function body:\n We first read the HTML source of a page with read_html. We can then use a combination of SelectorGadget with the \u0026ldquo;View page source\u0026rdquo; functionality of your browser to select the links on the page. Here we find that they belong to class link. We use the html_attr function here to extract the link path rather than the link text. The name = \u0026quot;href\u0026quot; specifies that we want the path attribute. (Anatomy of an HTML link: \u0026lt;a href=\u0026quot;https://www.something.com\u0026quot;\u0026gt;Link text seen on page\u0026lt;/a\u0026gt;). The remainder of the function subsets the extracted links to only those that pertain to the monster pages (removing links like the home page). Printing the output indicates that these links are only relative links, so we append the base URL to create absolute links (abs_links).  Finally, we can loop through all pages of results to get the hundreds of pages for the individual monsters:\n## Loop through all pages all_monster_urls \u0026lt;- lapply(seq_len(num_pages), function(i) { url \u0026lt;- paste0(\u0026quot;https://www.dndbeyond.com/monsters?page=\u0026quot;, i) get_monster_links(url) }) %\u0026gt;% unlist  Step 2: Use RSelenium to access pages behind login  In Step 1, we looped through pages of tables to get the URLs for pages that contain detailed information on individual monsters. Great! We can visit each of these pages and just do some more rvest work to scrape the details! Well\u0026hellip; not immediately. Most of these monster pages can only be seen if you have paid for the corresponding digital books and are logged in. DnD Beyond uses Twitch for authentication which involves a redirect. This redirect made it way harder for me to figure out what to do. It was like I had been thrown into the magical, mysterious, and deceptive realm of the Feywild where I frantically invoked Google magicks to find many dashed glimmers of hope but luckily a solution in the end.\nWhat did not work  It\u0026rsquo;s helpful for me to record what things I tried and failed so I can remember my thought process. Hopefully, it saves you wasted effort if you\u0026rsquo;re ever in a similar situation.\n Using rvest\u0026rsquo;s page navigation abilities did not work. I tried the following code:  url \u0026lt;- \u0026quot;https://www.dndbeyond.com/login\u0026quot; session \u0026lt;- html_session(url) url \u0026lt;- follow_link(session, \u0026quot;Login\u0026quot;)  But I ran into an error:\nError in curl::curl_fetch_memory(url, handle = handle) : Could not resolve host: NA   Using rvest\u0026rsquo;s basic authentication abilities did not work. I found this tutorial on how to send a username and password to a form with rvest. I tried hardcoding the extremely long URL that takes you to a Twitch authentication page, sending my username and password as described in the tutorial, and following [this Stack Overflow suggestion] to create a fake login button since the authentication page had an unnamed, unlabeled \u0026ldquo;Submit\u0026rdquo; input that did not seem to conform to rvest\u0026rsquo;s capabilities. I got a 403 error.  What did work  Only when I stumbled upon this Stack Overflow post did I learn about the RSelenium package. Selenium is a tool for automating web browsers, and the RSelenium package is the R interface for it.\nI am really grateful to the posters on that Stack Overflow question and this blog post for getting me started with RSelenium. The only problem is that the startServer function used in both posts is now defunct. When calling startServer, the message text informs you of the rsDriver function.\nStep 2a: Start automated browsing with rsDriver  The amazing feature of the rsDriver function is that you do not need to worry about downloading and installing other sofware like Docker or phantomjs. This function works right out of the box! To start the automated browsing, use the following:\nrd \u0026lt;- rsDriver(browser = \u0026quot;chrome\u0026quot;) rem_dr \u0026lt;- rd[[\u0026quot;client\u0026quot;]]  When you first run rsDriver, status messages will indicate that required files are being downloaded. After that you will see the status text \u0026ldquo;Connecting to remote server\u0026rdquo; and a Chrome browser window will pop open. The browser window will have a message beneath the search bar saying \u0026ldquo;Chrome is being controlled by automated test software.\u0026rdquo; This code comes straight from the example in the rsDriver help page.\nStep 2b: Browser navigation and interaction  The rem_dr object is what we will use to navigate and interact with the browser. This navigation and interaction is achieved by accessing and calling functions that are part of the rem_dr object. We can navigate to a page using the $navigate() function. We can select parts of the webpage with the $findElement() function. Once these selections are made, we can interact with the selections by\n Sending text to those selections with $sendKeysToElement() Sending key presses to those selections with $sendKeysToElement() Sending clicks to those selections with $clickElement()  All of these are detailed in the RSelenium Basics vignette, and further examples are in the Stack Overflow and blog post I mentioned above.\nThe code below shows this functionality in action:\nurl \u0026lt;- \u0026quot;https://www.dndbeyond.com/login\u0026quot; rem_dr$navigate(url) # Navigate to login page rem_dr$findElement(using = \u0026quot;css selector\u0026quot;, value = \u0026quot;.twitch-button\u0026quot;)$clickElement() # Click the \u0026quot;Login with Twitch\u0026quot; button ## Manually enter username and password here rem_dr$findElement(using = \u0026quot;css selector\u0026quot;, value = \u0026quot;.js-authorize-text\u0026quot;)$clickElement() # Click the \u0026quot;Authorize\u0026quot; button to continue logging in  Note: Once the Chrome window opens, you can finish the login process programatically as above or manually interface with the browser window as you would normally. This can be safer if you don\u0026rsquo;t want to have a file with your username and password saved anywhere.\nStep 2c: Extract page source  Now that we have programatic control over the browser, how do we interface with rvest? Once we navigate to a page with $navigate(), we will need to extract the page\u0026rsquo;s HTML source code to supply to rvest::read_html. We can extract the source with $getPageSource():\nrem_dr$navigate(url) page \u0026lt;- read_html(rem_dr$getPageSource()[[1]])  The subset [[1]] is needed after calling rem_dr$getPageSource() because $getPageSource() returns a list of length 1. The HTML source that is read in can be directly input to rvest::read_html.\nExcellent! Now all we need is a function that scrapes the details of a monster page and loop! In the following, we put everything together in a loop that iterates over the vector of URLs (all_monster_urls) generated in Step 1.\nWithin the loop we call the custom scrape_monster_page function to be discussed below in Step 3. We also include a check for purchased content. If you try to access a monster page that is not part of books that you have paid for, you will be redirected to a new page. We perform this check with the $getCurrentUrl() function, filling in a missing value for the monster information if we do not have access. The Sys.sleep at the end can be useful to avoid overloading your computer or if rate limits are a problem.\nmonster_info \u0026lt;- vector(\u0026quot;list\u0026quot;, length(all_monster_urls)) for (i in seq_along(all_monster_urls)) { url \u0026lt;- all_monster_urls[i] rem_dr$navigate(url) page \u0026lt;- read_html(rem_dr$getPageSource()[[1]]) ## If content has not been unlocked, the page will redirect curr_url \u0026lt;- rem_dr$getCurrentUrl()[[1]] if (curr_url == url) { monster_info[[i]] \u0026lt;- scrape_monster_page(page) } else { monster_info[[i]] \u0026lt;- NA } Sys.sleep(2) cat(i, \u0026quot; \u0026quot;) }  Step 3: Write a function to scrape an individual page  The last step in our scraping endeavor is to write the scrape_monster_page function to scrape data from an individual monster page. You can view the full function on GitHub. I won\u0026rsquo;t go through every aspect of this function here, but I\u0026rsquo;ll focus on some principles that appear in this function that I\u0026rsquo;ve found to be useful in general when working with rvest.\nPrinciple 1: Use SelectorGadget AND view the page\u0026rsquo;s source  As useful as SelectorGadget is for finding the correct CSS selector, I never use it alone. I always open up the page\u0026rsquo;s source code and do a lot of Ctrl-F to quickly find specific parts of a page. For example, when I was using SelectorGadget to get the CSS selectors for the Armor Class, Hit Points, and Speed attributes, I saw the following:\nI wanted to know if there were further subdvisions of the areas that the .mon-stat-block__attribute selector had highlighted. To do this, I searched the source code for \u0026ldquo;Armor Class\u0026rdquo; and found the following:\n\u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-label\u0026quot;\u0026gt;Armor Class\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-value\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data-value\u0026quot;\u0026gt; 19 \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data-extra\u0026quot;\u0026gt; (Natural Armor) \u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-label\u0026quot;\u0026gt;Hit Points\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data-value\u0026quot;\u0026gt; 207 \u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data-extra\u0026quot;\u0026gt; (18d12 + 90) \u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-label\u0026quot;\u0026gt;Speed\u0026lt;/span\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data\u0026quot;\u0026gt; \u0026lt;span class=\u0026quot;mon-stat-block__attribute-data-value\u0026quot;\u0026gt; 40 ft., fly 80 ft., swim 40 ft. \u0026lt;/span\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/div\u0026gt;  Looking at the raw source code allowed me to see that each line was subdivided by spans with classes mon-stat-block__attribute-label, mon-stat-block__attribute-data-value, and sometimes mon-stat-block__attribute-data-extra.\nWith SelectorGadget, you can actually type a CSS selector into the text box to highlight the selected parts of the page. I did this with the mon-stat-block__attribute-label class to verify that there should be 3 regions highlighted.\nBecause SelectorGadget requires hovering your mouse over potentially small regions, it is best to verify your selection by looking at the source code.\nPrinciple 2: Print often  Continuing from the above example of desiring the Armor Class, Hit Points, and Speed attributes, I was curious what I would obtain if I simply selected the whole line for each attribute (as opposed to the three subdivisions). The following is what I saw when I printed this to the screen:\n\u0026gt; page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute\u0026quot;) %\u0026gt;% html_text() [1] \u0026quot;\\n Armor Class\\n \\n \\n 19\\n \\n \\n \\n (Natural Armor) \\n \\n \\n \\n \u0026quot; [2] \u0026quot;\\n Hit Points\\n \\n \\n 207\\n \\n \\n (18d12 + 90)\\n \\n \\n \u0026quot; [3] \u0026quot;\\n Speed\\n \\n \\n 40 ft., fly 80 ft., swim 40 ft.\\n \\n \\n \\n \u0026quot;  A mess! A length-3 character vector containing the information I wanted but not in a very tidy format. Because I want to visualize and explore this data later, I want to do a little tidying up front in the scraping process.\nWhat if I just access the three subdivisions separately and rbind them together? This is not a good idea because of missing elements as shown below:\n\u0026gt; page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute-label\u0026quot;) %\u0026gt;% html_text() [1] \u0026quot;Armor Class\u0026quot; \u0026quot;Hit Points\u0026quot; \u0026quot;Speed\u0026quot; \u0026gt; page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute-data-value\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% trimws() [1] \u0026quot;19\u0026quot; \u0026quot;207\u0026quot; [3] \u0026quot;40 ft., fly 80 ft., swim 40 ft.\u0026quot; \u0026gt; page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute-data-extra\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% trimws() [1] \u0026quot;(Natural Armor)\u0026quot; \u0026quot;(18d12 + 90)\u0026quot;  For attribute-label, I get a length-3 vector. For attribute-data-value, I get a length-3 vector. For attribute-data-value, I only get a length-2 vector! Through visual inspection, I know that the third line \u0026ldquo;Speed\u0026rdquo; is missing the span with the data-extra class, but I don\u0026rsquo;t want to rely on visual inspection for these hundreds of monsters! Printing these results warned me directly that this could happen! Awareness of these missing items motivates the third principle.\nPrinciple 3: You will need loops  For the Armor Class, Hit Points, and Speed attributes, I wanted to end up with a data frame that looks like this:\n\u0026gt; attrs # A tibble: 3 x 3 label value extra \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; 1 Armor Class 19 (Natural Armor) 2 Hit Points 207 (18d12 + 90) 3 Speed 40 ft., fly 80 ft., swim 40 ft. NA  This data frame has properly encoded missingness. To do this, I needed to use a loop as shown below.\n## Attributes: AC, HP, speed attr_nodes \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute\u0026quot;) attrs \u0026lt;- do.call(rbind, lapply(attr_nodes, function(node) { label \u0026lt;- node %\u0026gt;% select_text(\u0026quot;.mon-stat-block__attribute-label\u0026quot;) data_value \u0026lt;- node %\u0026gt;% select_text(\u0026quot;.mon-stat-block__attribute-data-value\u0026quot;) data_extra \u0026lt;- node %\u0026gt;% select_text(\u0026quot;.mon-stat-block__attribute-data-extra\u0026quot;) %\u0026gt;% replace_if_empty(NA) tibble(label = label, value = data_value, extra = data_extra) }))  The code below makes use of two helper functions that I wrote to cut down on code repetition:\n select_text to cut down on the repetitive page %\u0026gt;% html_nodes %\u0026gt;% html_text  select_text \u0026lt;- function(xml, selector, trim = TRUE) { text \u0026lt;- xml %\u0026gt;% html_nodes(selector) %\u0026gt;% html_text if (trim) { text \u0026lt;- text %\u0026gt;% trimws } text }   replace_if_empty to repace empty text with NA  replace_if_empty \u0026lt;- function(text, to) { if (length(text)==0) { text \u0026lt;- to } text }  I first select the three lines corresponding to these three attributes with\nattr_nodes \u0026lt;- page %\u0026gt;% html_nodes(\u0026quot;.mon-stat-block__attribute\u0026quot;)  This creates a list of three nodes (pieces of the webpage/branches of the HTML tree) corresponding to the three lines of data:\n\u0026gt; attr_nodes {xml_nodeset (3)} [1] \u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt;\\n \u0026lt;span class=\u0026quot;mon-sta ... [2] \u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt;\\n \u0026lt;span class=\u0026quot;mon-sta ... [3] \u0026lt;div class=\u0026quot;mon-stat-block__attribute\u0026quot;\u0026gt;\\n \u0026lt;span class=\u0026quot;mon-sta ...  We can chain together a series of calls to html_nodes. I do this in the subsequent lapply statement. I know that each of these nodes contains up to three further subdivisions (label, value, and extra information). In this way I can make sure that these three pieces of information are aligned between the three lines of data.\nNearly all of the code in the scrape_monster_page function repeats these three principles, and I\u0026rsquo;ve found that I routinely use similar ideas in other scraping I\u0026rsquo;ve done with rvest.\nSummary  This is a long post, but a few short take-home messages suffice to wrap ideas together:\n rvest is remarkably effective at scraping what you need with fairly concise code. Following the three principles above has helped me a lot when I\u0026rsquo;ve used this package. rvest can\u0026rsquo;t do it all. For scraping tasks where you wish that you could automate clicking and typing in the browser (e.g. authentication settings), RSelenium is the package for you. In particular, the rsDriver function works right out of the box (as far as I can tell) and is great for people like me who are loath to install external dependencies.  Happy scraping!\n","date":1533859200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533859200,"objectID":"51aa69d3f128a44ceeafd8b6854fec96","permalink":"/post/dnd-scraping-rvest-rselenium/","publishdate":"2018-08-10T00:00:00Z","relpermalink":"/post/dnd-scraping-rvest-rselenium/","section":"post","summary":"I love Dungeons and Dragons. I am also a data-loving statistician. At some point, these worlds were bound to collide.\nFor those unfamiliar with Dungeons and Dragons (DnD), it is a role-playing game that is backed by an extraodinary amount of data.","tags":["rvest","rselenium","dnd"],"title":"Dungeons and Dragons Web Scraping with rvest and RSelenium","type":"post"},{"authors":null,"categories":["causal inference","reading"],"content":"I just finished reading The Book of Why by Judea Pearl and Dana Mackenzie, and I really enjoyed it. I had been wanting to read it for some time now because I know very little about methodology relating to causal diagrams and structure learning. The book provides an overview of the main ideas that formed and historical events that led up to what Pearl calls the \u0026ldquo;Causal Revolution\u0026rdquo;, a burgeoning of the direct interrogation of causation as opposed to its implicit renouncement in science for a period before then. Much of the causal inference methodology that Pearl discusses in the book is his own, namely that of causal diagrams and do-calculus. In light of his own methods, he also discusses techniques that are popular in disciplines such as psychology and economics. He also discusses another major framework for causal inference, the Rubin causal model. In reflecting on the book, I found it useful to organize my thoughts according to major themes I saw in the book.\nTable of Contents  Language and diagrams History Cognition Teaching Conclusions  Language and diagrams  Right from the introduction, Pearl emphasizes the importance of language in science:\n My emphasis on language also comes from a deep conviction that language shapes our thoughts. You cannot answer a question that you cannot ask, and you cannot ask a question that you have no words for. (p. 10)\n I love this quote because it echoes how essential careful language is for humanity\u0026rsquo;s progress. We cannot understand an idea without expressing it in some language in our own minds. We cannot transfer this understanding faithfully to others without carefully crafting language. This crafting of language affects how others understand, consume, act upon, and transfer the idea. And the cycle repeats. Essentially, language governs our intellectual progeny, which has profound scientific, moral, and cultural implications. There is even a growing body of scientific evidence regarding specific ways in which language shapes our thoughts. A TED talk by Lera Boroditsky discusses some of these.\nWhy does Pearl emphasize the importance of language for causal inference? It has to do with precision, and it reminds me of a scene from Lois Lowry\u0026rsquo;s The Giver.\n \u0026ldquo;What is it, Jonas?\u0026rdquo; his father asked.\nHe made himself say the words, though he felt flushed with embarrassment. He had rehearsed them in his mind all the way home from the Annex.\n\u0026ldquo;Do you love me?\u0026rdquo;\nThere was an awkward silence for a moment. Then Father gave a little chuckle. \u0026ldquo;Jonas. You, of all people. Precision of language, please!\u0026rdquo;\n Earlier in the book it is revealed that the reason for the strict adherence to precision of language in Jonas\u0026rsquo;s community is to avoid unintentional lies that can come about through exaggeration or misinterpretation. We learn quickly in the story that this precision of language creates a dystopia, devoid of true feeling and the emotions that make up a beautiful human life.\nA bleak picture in the case of The Giver, but the existence of a language that allows for precise expression is indispensable when it comes to science! Consider the following epidemiological investigation: we want to know how the consumption of red meat influences risk for colon cancer. There are two questions that we might think of quickly.\n Does eating red meat cause an increase in colon cancer risk? How much red meat consumption is needed to increase the risk of colon cancer?  My impression is that the first question is how the majority of the public perceives a causal effect. Does exposure cause outcome? Pearl explains that this conventional way of thinking about causal analysis is really not the goal of the field at all:\n Many people still make Niles\u0026rsquo;s mistake of thinking that the goal of causal analysis is to prove that X is a cause of Y or else to find the cause of Y from scratch. That is the problem of causal discovery. (p. 79)\n (Niles was a critic of path analysis/causal diagrams.) The goal of causal analysis is to quantitatively estimate causal effects while fully capturing the state of the analyst\u0026rsquo;s current knowledge. The full capturing of current knowledge is achieved by drawing a causal diagram.\nThe second question, though quantitative, is still imprecise. It gets more at the estimation goal of causal analysis, but its imprecision leads to individual interpretations of the best way to proceed (essentially researcher degrees of freedom). Certainly there will be differences between individuals in terms of what they feel is the current state of knowledge on a subject. That is, experts may disagree on their causal diagrams. This disagreement is ok as long as their working set of assumptions (the causal diagrams) are made explicit. Usually when researchers ask a causal question like number 2, researcher degrees of freedom abound in both the variables considered and the manner in which the variables are handled in the analytic method.\nBoth of these issues can be avoided by using causal diagrams and the accompanying language of do-calculus. Causal diagrams are a means of precisely representing current knowledge. Do-calculus consists of a set of rules that allow us to express a causal quantity that we want to estimate in terms of quantities that can be computed from data. That is, it is a set of rules that allows us to express the effect of an intervention in terms of observational data quantities. An interventional effect is specified with the do-operator as with $P(Y \\mid do(X))$ to indicate a deliberate intervention. This is usually quite different from the observational quantity $P(Y \\mid X)$ as a classic confounding example illustrates. Let $Y$ denote reading ability and $X$ denote shoe size. We all know that age confounds the relationship as it is a cause of both shoe size and reading ability (provided the individual benefits from education). Were it not completely insane, intervening on shoe size would not change $P(Y \\mid do(X))$, but the observational quantity $P(Y \\mid X)$ does change as $X$ changes.\nIt was not intuitive for me that the effect of an intervention that has not actually been carried out could be estimated from observational data, but Pearl builds up these ideas in The Book of Why to explain (in Chapter 7) that 3 rules of do-calculus suffice for determining if a particular causal effect can be estimated from observation data given a causal diagram. This blog post gives more technical details about causal diagrams and do-calculus, and the introductory paper cited in that post explains the 3 rules in detail.\nThe combination of causal diagrams and do-calculus is a powerful idea for me because of the precision of language that it offers for making causal queries. We first must lay our assumptions bare with a causal diagram. This was not a hard point to sell me on because I am already a firm believer in the power of network methods to organize domain knowledge. Do-calculus on the other hand is quite surprising. Still, the 3 rules provide clear guidelines on how to express a causal effect from observational data, and this clarity in allowable expressions is for me a compelling motivator for their use. Pearl mentions in the book that these rules have been algorithmized, and in my brief searching, I have found the R package called daggity that seems to implement the rules of do-calculus.\nHistory  Another major theme of this book is understanding history. One entire chapter is devoted to tracing the history of how causal inference came to be. Pearl and Mackenzie start by recounting the tale of Francis Galton, a British scientist who was on a quest to understand the genetic determinants of features like height and intelligence. He eventually stumbled upon the phenomenon of regression to the mean and saw it as a physical, casual process because it was able to reconcile some peculiar features of models that he had developed for height distributions across generations. However, he eventually grew dissatisfied with the idea after finding that the phenomenon persisted regardless of which variable was treated as the causal agent. He was never able to resolve his initial causal queries about genetic determinants, but he did pass on ideas of scatterplots and correlation to future generations of statisticians. In particular, Karl Pearson took to the idea of correlation quite excitedly and came to eschew ideas of causality, which he viewed as imprecise and vague in contrast to his clean, mathematized correlation coefficient. Such was a major force behind the lack of causality research in statistics for some time. Pearl and Mackenzie end this historical chapter with the tale of Sewall Wright, who seems to have been one of the first to come up with the idea of using causal diagrams. Through Wright\u0026rsquo;s tale we see a reemergence of the willingness to study casuality rigorously and quantitatively.\nThis historical discussion is fascinating because it allows us (with our hindsight goggles on) to understand why research progressed the way that it did. Through understanding the personalities and culture of these historical figures, we can understand why certain ideas were pursued, why shortcomings arose, and hopefully mediate ourselves to be better scientists because of this understanding.\nCognition  The importance of an awareness of human cognition is also a recurring idea in the book. Pearl motivates his journey through causal inference with his interests in artificial intelligence, and he claims that causal inference methods should ideally try to emulate the powerful causal reasoning faculties within our own minds that stem from simply asking the question: why? I like the apparent simplicity of this. It is easy to see how asking this question could give rise to causal diagrams. Each \u0026ldquo;because\u0026rdquo; becomes a directed connection from nodes that represent variables in our \u0026ldquo;because\u0026rdquo; statement. Still, at the same time, I wondered: should there not be some higher standard to which causal inference methods should aspire? Why simply aim to replicate human reasoning? Shouldn\u0026rsquo;t we strive for our methods to achieve something more than just human reasoning in some sense? After thinking about this, I feel that these goals are too lofty. We humans are limited by our capabilities, so even if causal inference methods could achieve beyond-human reasoning, we wouldn\u0026rsquo;t know that they were. Given a method that produces some results, we would still evaluate the method by asking, \u0026ldquo;Do those results make sense?\u0026rdquo; We would still be using our (powerful) causal reasoning capabilities to make sense of the results generated by the method. Thus even if some deeper meaning was somehow conveyed by the method, the meaning we would be able to extract from it is limited by the framework of our understanding. I feel that Pearl\u0026rsquo;s claims about using human reasoning as a gold standard for causal inference methods is reasonable. This thinking also reaffirms my belief in the importance of understanding how humans interact with the tools we develop, such as through the fields of ergonomics, human-computer interaction, human-data interaction.\nAn awareness of human cognition is explored most extensively in a chapter on several paradoxes: the Monty Hall problem, Berkson\u0026rsquo;s paradox, Simpson\u0026rsquo;s paradox, and Lord\u0026rsquo;s paradox. I had actualy never heard of Berkson\u0026rsquo;s paradox, but it is the appearance of an association in a subpopulation that is not seen in the general population. Pearl explores all of these paradoxes in light of causal diagrams, and I actually did find it helpful to view these problems with this causal lens. The causal diagrams were useful in generalizing the structure of the situations governing the paradoxes, which I think is helpful in recognizing when they occur. Further, Pearl lays forth a reasonable set of criteria for dealing with paradoxes:\n Any claim to resolve a paradox (especially one that is decades old) should meet some basic criteria. First, as I said above in connection with the Monty Hall paradox, it should explain why people find the paradox surprising or unbelievable. Second, it should identify the class of scenarios in which the paradox can occur. Third, it should inform us of scenarios, if any, in which the paradox cannot occur. Finally, when the paradox does occur, and we have to make a choice between two plausible yet contradictory statements, it should tell us which statement is correct. (p. 202)\n Teaching  By its nature, the book aims to inform readers about the development of and about central ideas in causal inference, but teaching and pedagogy are not direct themes of the book. That being said, I was amazed at how appropriate the writing of the book is for a classroom textbook. There are a lot of great thought experiments, historical examples, and activities to engage students at the undergraduate level and beyond. In particular, the chapters that recount the evolution of the smoking-lung cancer debate (Chapter 5) and that explore \u0026ldquo;statistical\u0026rdquo; paradoxes through a causal lens (Chapter 6) are great sources of classroom content.\nConclusions  I highly recommend this book to anyone who cares about science. Even if causal inference isn\u0026rsquo;t an area of interest for you, the ideas in this book are important for understanding the causal research that we otherwise consume or hear about. Pearl is very invested in these ideas, so the language in the book is very enthusiastically in favor of these methods. I can see how this might irritate some readers, but I found that the ideas he presented were compelling and interesting in their own right. Certainly these methods are not a panacea, but I do believe that they can be quite useful. Reading the book has motivated me to continue learning about these topics, and I hope that I can eventually fully understand the answers to some questions I was left with:\n Is there no reconciliation at all for Rubin causal model type methods and do-calculus methods? How can causal diagrams and do-calculus be used to study networks that evove with time? How is interference between units handled? How can we measure the causal effect of several variables simultaneously? I have heard of edges being random variables in the graphical model literature. (i.e. Arrows can point to arrows.) Is this part of the do-calculus framework?  ","date":1532476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532476800,"objectID":"2cd438588184d687c626d55f16ba6e01","permalink":"/post/book-of-why/","publishdate":"2018-07-25T00:00:00Z","relpermalink":"/post/book-of-why/","section":"post","summary":"I just finished reading The Book of Why by Judea Pearl and Dana Mackenzie, and I really enjoyed it. I had been wanting to read it for some time now because I know very little about methodology relating to causal diagrams and structure learning.","tags":["causal diagrams"],"title":"The Book of Why","type":"post"},{"authors":null,"categories":["R"],"content":"I recently migrated my personal website and Wordpress blog to blogdown. As an academic, it was natural to use the Academic theme. The blogdown package made the conversion fairly straighforward, but I still had to spend some time figuring out how to work with this Hugo theme.\nThe source and rendered files for my website are available on GitHub:\n Public, rendered site: the public directory within my blogdown/Hugo project Hugo content and source files: all files and directories within my blogdown/Hugo project (i.e. TOML files, archetypes/, content/, data/, layouts/, static/, themes/)  Table of Contents  Resources Start with config.toml Full content RSS feeds for categories Modifying the Contact section Adding a CV Version control  Resources  This post contains a minimal set of notes that I used to configure specfic parts of the Academic theme and is not a full tutorial on starting a blogdown website. The references and tutorials below are helpful for the initial setup of your site.\n blogdown book by Yihui Xie, Amber Thomas, and Alison Presmanes Hill Up and running with blogdown by Alison Presmanes Hill Making a Website Using Blogdown, Hugo, and GitHub pages by Amber Thomas  Start with config.toml  The key-value pairs in config.toml are pretty straightforward, and I was able to very quickly fill in basic information to populate the home page. The places I\u0026rsquo;ll mention next are ones where I had to spend a little more time.\nColor theme [params] # Color theme. # Choose from `default`, `ocean`, `forest`, `coffee`, `dark`, or `1950s`. color_theme = \u0026quot;custom\u0026quot;  This sets the color scheme for your site. I changed the theme to \u0026ldquo;custom\u0026rdquo; and made created a file called custom.toml in themes/hugo-academic/data/themes/. I have the following in my custom.toml file:\n# Theme metadata name = \u0026quot;custom\u0026quot; # Is theme light or dark? light = true # Primary primary = \u0026quot;#328cc1\u0026quot; primary_light = \u0026quot;#328cc1\u0026quot; primary_dark = \u0026quot;#DA2536\u0026quot; # Menu menu_primary = \u0026quot;#494949\u0026quot; menu_text = \u0026quot;#fff\u0026quot; menu_text_active = \u0026quot;#328cc1\u0026quot; menu_title = \u0026quot;#fff\u0026quot; # Backgrounds background = \u0026quot;#fff\u0026quot; home_section_odd = \u0026quot;#fff\u0026quot; home_section_even = \u0026quot;#f7f7f7\u0026quot;  The \u0026ldquo;Primary\u0026rdquo; section changes the color of links and icons depending on whether you want a dark or light-colored theme. The \u0026ldquo;Menu\u0026rdquo; section changes the colors in the top menu bar. The \u0026ldquo;Backgrounds\u0026rdquo; section changes the color of the section panels on the first page.\nhighlight.js In this section, you can configure the languages for which you want to support syntax highlighting. As mentioned in the comments in this section of config.toml, you can visit https://cdnjs.com/libraries/highlight.js/ to see the list of languages supported (URls ending in languages/LANGUAGE_NAME.min.js). You\u0026rsquo;ll also see a list of color schemes (URLs ending in styles/STYLE_NAME.min.css). I wanted to know what these color schemes looked like, so I searched and found https://highlightjs.org/static/demo/.\nFull content RSS feeds for categories  When I first started building my site with the Academic theme, I noticed that most of my RSS feeds (e.g. https://lmyint.github.io/post/index.xml, https://lmyint.github.io/categories/r/index.xml) contained only a brief summary of my posts in the description tags as opposed to the full post content. Only my home page RSS feed (https://lmyint.github.io/index.xml) had full content of posts.\nFollowing the advice here by changing the outputs in config.toml to the TOML below did not fix the issue.\n[outputs] home = [ \u0026quot;HTML\u0026quot;, \u0026quot;CSS\u0026quot;, \u0026quot;RSS\u0026quot; ] section = [ \u0026quot;HTML\u0026quot;, \u0026quot;RSS\u0026quot; ] taxonomy = [ \u0026quot;HTML\u0026quot;, \u0026quot;RSS\u0026quot; ] taxonomyTerm = [ \u0026quot;HTML\u0026quot;, \u0026quot;RSS\u0026quot; ]  The fix: If you would like to contribute certain posts to a content aggregator that requires full post content on the RSS feed (such as R-Bloggers), do the following:\n Put these posts in one category (not tag) Go to https://gohugo.io/templates/rss/#the-embedded-rss-xml Look in the third row of the table: Taxonomy list in categories Create layouts/categories/category.rss.xml and use the default RSS template at the bottom of the page replacing  \u0026lt;description\u0026gt;{{ .Summary | html }}\u0026lt;/description\u0026gt;  with\n\u0026lt;description\u0026gt;{{ .Content | html }}\u0026lt;/description\u0026gt;  After this, the RSS feeds for your category pages should have full post content.\nModifying the Contact section  By default, the Contact section of the page will display certain items in the params table of your config.toml file. With the TOML below, the Contact section would only contain my e-mail address.\n[params] # Some other stuff... email = \u0026quot;lmyint@macalester.edu\u0026quot; address = \u0026quot;\u0026quot; office_hours = \u0026quot;\u0026quot; phone = \u0026quot;\u0026quot; skype = \u0026quot;\u0026quot; telegram = \u0026quot;\u0026quot;  I wanted to modify the Contact section to also show my Twitter handle, so I changed the TOML to the following.\n[params] # Some other stuff... email = \u0026quot;lmyint@macalester.edu\u0026quot; address = \u0026quot;\u0026quot; office_hours = \u0026quot;\u0026quot; phone = \u0026quot;\u0026quot; skype = \u0026quot;\u0026quot; telegram = \u0026quot;\u0026quot; twitter = \u0026quot;lesliemyint\u0026quot;  I also had to update themes/hugo-academic/layouts/partials/widgets/contact.html. I duplicated the section of the HTML that displays the e-mail address parameter:\n{{ with $.Site.Params.email }} \u0026lt;li\u0026gt; \u0026lt;i class=\u0026quot;fa-li fa fa-envelope fa-2x\u0026quot; aria-hidden=\u0026quot;true\u0026quot;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span id=\u0026quot;person-email\u0026quot; itemprop=\u0026quot;email\u0026quot;\u0026gt; {{- if $autolink }}\u0026lt;a href=\u0026quot;mailto:{{ . }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt;{{ else }}{{ . }}{{ end -}} \u0026lt;/span\u0026gt; \u0026lt;/li\u0026gt; {{ end }}  And I modified it to access the Twitter parameter ($.Site.Params.twitter), use the Twitter icon (class=\u0026quot;fa-twitter\u0026quot;), and link to the Twitter website.\n{{ with $.Site.Params.twitter }} \u0026lt;li\u0026gt; \u0026lt;i class=\u0026quot;fa-li fa fa-twitter fa-2x\u0026quot; aria-hidden=\u0026quot;true\u0026quot;\u0026gt;\u0026lt;/i\u0026gt; \u0026lt;span\u0026gt; \u0026lt;a href=\u0026quot;https://twitter.com/{{ . }}\u0026quot;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; \u0026lt;/span\u0026gt; \u0026lt;/li\u0026gt; {{ end }}  Adding a CV  I use an HTML template for my CV and wanted to link to both the HTML and PDF versions.\nFirst, I added a CV section to my top menu bar by adding the following TOML to config.toml:\n[[menu.main]] name = \u0026quot;CV\u0026quot; url = \u0026quot;#cv\u0026quot; weight = 5  Next, I created a directory called cv/ within the content/ directory and added the HTML and PDF versions of my CV, index.html and cv.pdf respectively to content/cv/. Because my HTML CV relies on a stylesheet (cv.css), I added it to static/css/, and I link to it in index.html with the following:\n\u0026lt;link type=\u0026quot;text/css\u0026quot; rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;../css/cv.css\u0026quot;\u0026gt;  The relative linking (../css/cv.css) looks as such because the directory structure that is generated in public/ looks like this:\n|-public/ |---index.html |---css/ |------cv.css |---cv/ |------cv.pdf |------index.html  Last, I created cv.md in content/home/ by duplicating the teaching.md file that comes with the theme by default. You can view my cv.md file on GitHub. The main hurdle in linking external resources is figuruing out the correct relative paths to these files. Again, looking at the generated directory structure above, we have to specify paths relative to index.html. So I include the following in cv.md.\nMy CV is available in [HTML](cv/) or [PDF](cv/cv.pdf) form.  Version control  My website is hosted with GitHub pages, and the associated repository only contains the file in the public directory of my Hugo project.\nI used the Host on GitHub tutorial to figure out that the public directory can be set up as a git submodule within an enclosing git repository containing source files. The enclosing git repository for my website is available at: https://github.com/lmyint/personal_site.\n","date":1530057600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530057600,"objectID":"65e2b490dc8f3774bb13bbac9321de06","permalink":"/post/hugo-academic-tips/","publishdate":"2018-06-27T00:00:00Z","relpermalink":"/post/hugo-academic-tips/","section":"post","summary":"I recently migrated my personal website and Wordpress blog to blogdown. As an academic, it was natural to use the Academic theme. The blogdown package made the conversion fairly straighforward, but I still had to spend some time figuring out how to work with this Hugo theme.","tags":["blogdown"],"title":"Tips for using the Hugo academic theme","type":"post"},{"authors":null,"categories":["Opinion"],"content":"I love Game of Thrones. I particularly liked this mini-speech from Petyr Baelish earlier in Season 7:\n Don’t fight in the North or the South. Fight every battle everywhere, always, in your mind. Everyone is your enemy, everyone is your friend. Every possible series of events is happening all at once. Live that way and nothing will surprise you. Everything that happens will be something that you’ve seen before.\n As I let my mind wander away from work for a bit today, I realized that this is a wonderful quote about science!\nFight every battle everywhere, always, in your mind Baelish is a shrewd, obsessive planner. In planning for everything that could possibly happen, he always seems to be prepared, get what he wants, and stay alive. Just like staying alive in Westeros in positions of power, doing good science can be quite difficult because we are set adrift in extremely complex systems. There are so many paths that that can be followed to answer a research question (including the formulation of the question itself!), and all could be the subject of an intellectual battle with a critic. Studying the effect of yearly bonuses for teachers on long-term student outcomes? How do you define long-term? What outcomes will you measure and how? How will you prevent dropout? How do you make differing bonus amounts comparable for teachers who differ in terms of what they teach, where they live, what composition of students they teach from year to year? How would you even define \u0026ldquo;composition of students\u0026rdquo;? Also why study student outcomes as opposed to community outcomes? There is no way that a single study could address all of these concerns, or the ones that I couldn\u0026rsquo;t think of, but these concerns need to be thought about because they need to be answered for us to have any hope of meaningful, actionable conclusions. Just the act of forecasting these hypothetical intellectual battles can motivate the design of better studies.\nEveryone is your enemy, everyone is your friend Baelish is calculating and knows how effective people can be in various contexts. It can be helpful to think of everyone in the scientific community as your enemy—enemies ready to question every aspect of your work and find every possible hole—but only if it indeed motivates you to do those very things. Only by heavily scrutinizing our own work can we make ourselves the best scientists possible. Acknowledging limitations in private and subsequently making them known to others is key to moving the state of knowledge forward. I do want to de-emphasize any paranoid or hateful connotations of this quote though! Some people in my field take the \u0026ldquo;everyone is your enemy\u0026rdquo; part too seriously and critique others in inflammatory ways.\nNow the friends part\u0026hellip;this probably works out better for science than for Baelish. Scientists form a community, and ideally sharing everything about our work would facilitate a team effort to find even more limitations and address them fruitfully to get leaps and bounds closer to useful answers. But my impression is that things generally don\u0026rsquo;t happen this way. Groups work somewhat in isolation on different aspects of a problem. Perhaps consortia try to harmonize efforts in some respects, but useful information is still needed from external sources and not able to be integrated easily. Just as it is hard in Westeros to find good allies, it can be difficult in science to find good collaborators. But when it does happen, great deeds are in the works.\nEvery possible series of events is happening all at once In Westeros, livelihoods dance on the whims of nobles and on the breath of armies that can be traded with coin coffers or decimated in an afternoon. This creates a palpable urgency for Baelish to always stay ahead of the game. This immediacy isn\u0026rsquo;t really felt in science. We don\u0026rsquo;t gamble with our lives when we submit a paper and wait for the review process to unfold. I think that the scientific community is lured to progress slowly with our recognition system favoring large numbers of publications. Researchers who have large projects are incentivized to break the project up into several publications. I don\u0026rsquo;t think this is necessarily bad if the scientists have actually completed this larger body of research. The flaw I see is if it incentivizes scientists to publish work that isn\u0026rsquo;t as complete out of time pressure and fail to follow up with more complete validation because they feel the validation work isn\u0026rsquo;t \u0026ldquo;enough\u0026rdquo; for its own publication. There is some effort to recognize replication attempts, but I wish that there were more urgency and incentive to conduct more complete studies the first time around because it sets the baseline higher for future work.\n","date":1502844380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1502844380,"objectID":"b3fc9aa71b04035826b6f0cdcbe91326","permalink":"/post/fight-every-battle/","publishdate":"2017-08-16T00:46:20Z","relpermalink":"/post/fight-every-battle/","section":"post","summary":"I love Game of Thrones. I particularly liked this mini-speech from Petyr Baelish earlier in Season 7:\n Don’t fight in the North or the South. Fight every battle everywhere, always, in your mind.","tags":[],"title":"Fight every battle everywhere: this is science","type":"post"},{"authors":null,"categories":["R"],"content":"  Creating a Shiny application that enables user login can be useful for tailoring individual user experience and for analyzing user actions with profile-type data. With basic file I/O functions, it is possible to create a simple but insecure app that stores login names and passwords in text files. A much more secure alternative is to use an existing authentication system to handle login. I’m sure many of you have seen websites that allow you to login via Google or Facebook. I will outline here the steps needed to setup a “Login with Google” functionality on your Shiny app.\nStep 1: Install packages You will need the googleAuthR and googleID packages to allow for Google authentication and login. If you plan to publish your app on shinyapps.io, you’ll also need the shinyjs package to avoid a clunky “Disconnected from the server” message on logout. You can install these packages with\ninstall.packages(c(\u0026quot;googleAuthR\u0026quot;, \u0026quot;shinyjs\u0026quot;)) devtools::install_github(\u0026quot;MarkEdmondson1234/googleID\u0026quot;) It is important to install the googleID package with the command above to avoid an “Unable to retrieve package records” error when publishing your app (see here).\n Step 2: Setup Google APIs Setup a Google API project Make sure that you are logged into Google and visit the Google APIs project page. Click the “Create Project” link at the top and enter a name for the project (e.g. “myShinyApp”). After a few seconds, you will be redirected to the Google API manager. Click on the Google+ API link under “Social APIs” and click the “Enable” link at the top to activate the Google+ API.   Setup authentication credentials Click the “Credentials” link in the menu on the left. Navigate to the “OAuth consent screen” tab near the top. Fill in the “Product name shown to users” form with the name of your Shiny application. The information you provide in this tab populate the authentication screen that pops up when users click the “Login with Google” link in your app (example). Navigate to the “Credentials” tab at the top. On the “Create Credentials” dropdown menu, select “OAuth client ID” and select “Web application” for the application type. Fill in any descriptive name for this authentication client. In the redirect URLs field, fill in * the URL for your Shiny app (e.g. https://yourdomain.shinyapps.io/appName) * http://127.0.0.1:1221 This is to facilitate local development and testing of your app.\n After saving this information, a client ID and secret will pop up. Copy and paste these for use in your code later.    Step 3: Code Include the following code at the top of your app.R file to setup scopes for the relevant API functions you’ll be using and to specify the client ID and secret you received in step 8 above:\noptions(googleAuthR.scopes.selected = c(\u0026quot;https://www.googleapis.com/auth/userinfo.email\u0026quot;, \u0026quot;https://www.googleapis.com/auth/userinfo.profile\u0026quot;)) options(\u0026quot;googleAuthR.webapp.client_id\u0026quot; = \u0026quot;YOUR_CLIENT_ID\u0026quot;) options(\u0026quot;googleAuthR.webapp.client_secret\u0026quot; = \u0026quot;YOUR_CLIENT_SECRET\u0026quot;) Below is the shell of an app.R file that will create a login/logout button using Google authentication. I’ll explain the individual components afterward.\nui \u0026lt;- navbarPage( title = \u0026quot;App Name\u0026quot;, windowTitle = \u0026quot;Browser window title\u0026quot;, tabPanel(\u0026quot;Tab 1\u0026quot;, useShinyjs(), sidebarLayout( sidebarPanel( p(\u0026quot;Welcome!\u0026quot;), googleAuthUI(\u0026quot;gauth_login\u0026quot;) ), mainPanel( textOutput(\u0026quot;display_username\u0026quot;) ) ) ), tabPanel(\u0026quot;Tab 2\u0026quot;, p(\u0026quot;Layout for tab 2\u0026quot;) ) ) server \u0026lt;- function(input, output, session) { ## Global variables needed throughout the app rv \u0026lt;- reactiveValues( login = FALSE ) ## Authentication accessToken \u0026lt;- callModule(googleAuth, \u0026quot;gauth_login\u0026quot;, login_class = \u0026quot;btn btn-primary\u0026quot;, logout_class = \u0026quot;btn btn-primary\u0026quot;) userDetails \u0026lt;- reactive({ validate( need(accessToken(), \u0026quot;not logged in\u0026quot;) ) rv$login \u0026lt;- TRUE with_shiny(get_user_info, shiny_access_token = accessToken()) }) ## Display user\u0026#39;s Google display name after successful login output$display_username \u0026lt;- renderText({ validate( need(userDetails(), \u0026quot;getting user details\u0026quot;) ) userDetails()$displayName }) ## Workaround to avoid shinyaps.io URL problems observe({ if (rv$login) { shinyjs::onclick(\u0026quot;gauth_login-googleAuthUi\u0026quot;, shinyjs::runjs(\u0026quot;window.location.href = \u0026#39;https://yourdomain.shinyapps.io/appName\u0026#39;;\u0026quot;)) } }) } shinyApp(ui = ui, server = server) The login/logout button is created as part of the UI by calling the googleAuthUI function and supplying an ID:\ngoogleAuthUI(\u0026quot;gauth_login\u0026quot;) Use the same ID to call the Google authentication module with callModule. It is also possible to set the classes of the login and logout buttons. For styling purposes, I’ve set the classes of the login and logout buttons to be the same which renders the buttons as flat blue buttons with white text. By default, the logout button just has the btn class and is a standard silver button.\naccessToken \u0026lt;- callModule(googleAuth, \u0026quot;gauth_login\u0026quot;, login_class = \u0026quot;btn btn-primary\u0026quot;, logout_class = \u0026quot;btn btn-primary\u0026quot;) The userDetails object is a reactive expression that is a list of several pieces of information from the user’s Google profile (see the googleID example). Until the access token is generated, any output that depends on userDetails will instead display “not logged in.”\nuserDetails \u0026lt;- reactive({ validate( need(accessToken(), \u0026quot;not logged in\u0026quot;) ) rv$login \u0026lt;- TRUE with_shiny(get_user_info, shiny_access_token = accessToken()) }) If parts of the UI are to be rendered based on this information after user login, include a validate() command:\noutput$display_username \u0026lt;- renderText({ validate( need(userDetails(), \u0026quot;getting user details\u0026quot;) ) userDetails()$displayName }) Without the last piece of code using shinyjs, clicking the logout button would cause the app to be disconnected from the server. This results in a clunky, undesirable logout experience. This last piece of code redirects to the specified URL when the logout button is clicked.\nobserve({ if (rv$login) { shinyjs::onclick(\u0026quot;gauth_login-googleAuthUi\u0026quot;, shinyjs::runjs(\u0026quot;window.location.href = \u0026#39;https://yourdomain.shinyapps.io/appName\u0026#39;;\u0026quot;)) } })  Other considerations The steps above should help you quickly get started developing a Shiny application with Google login. The meat of the app will depend on your needs, but if you want to keep track of user information, consider using some online file system or database to map users’ Google IDs to your app’s own set of profile information.\n ","date":1483295069,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483295069,"objectID":"dbb0c35a5a21280f2e7dff3d49a1617b","permalink":"/post/shiny-app-with-google-login/","publishdate":"2017-01-01T18:24:29Z","relpermalink":"/post/shiny-app-with-google-login/","section":"post","summary":"Creating a Shiny application that enables user login can be useful for tailoring individual user experience and for analyzing user actions with profile-type data. With basic file I/O functions, it is possible to create a simple but insecure app that stores login names and passwords in text files.","tags":["Shiny","OAuth"],"title":"Creating a Shiny app with Google login","type":"post"},{"authors":null,"categories":["Education"],"content":"  I am currently a TA for an introductory biostatistics sequence at JHSPH where we teach students about the essentials of regression analysis. A great question that came up at office hours last week was, “What is likelihood?” I love this question because it is so fundamental to statistical thought, seems very intuitive, but actually abounds in nuance.\nI found my answer to the question to be rather unsatisfying: “Likelihood refers to how probable our collected data would be given the regression model that we’re currently fitting. The higher the likelihood that Stata reports, the more likely it is that we observe our data under the specified regression model.” Still not seeing the click, I added, “So essentially the higher the likelihood, the better the model is at fitting, at predicting the data. We’re using likelihood here as a means of measuring the predictive power of a regression model.” Some nodding.\nI’ve been thinking about this more, and the standard explanation of likelihood as being “under a certain model” is rather confusing. At least phrased in this way. Students are actually quite familiar with this idea in other settings. So if I could go back and answer the question again, I would want the conversation to go something like this:\n“So what is likelihood?”\n“Good question! Let me ask you this: what’s the probability of rolling a one on a six-sided die?”\n“One-sixth…”\n“Ah - what if I told you that it was actually 90%?”\nSilence or contemplation.\n“So why did you say one-sixth?”\n“Because there’s a single one out of the six sides.”\n“Right - so you assumed a model of a fair die. And using a model for a fair die, each number has a one-sixth chance of being rolled. I assumed a model of an exquisitely crafted loaded die where the chance of getting a one is 90% and the other five numbers is 2% each.”\n“Ok…”\n“We’re getting close to the meat of it! So I have a model in mind, and you have a model in mind…who’s right? The only way to hope to answer this is with data. Say I rolled this hypothetical die and got a 2, 3, 4, 6, and then a 2. What die model do you think is better?”\n“The fair one.”\n“Right - why?”\n“There wasn’t even a single one rolled, and your loaded die is supposed to have a 90% chance to roll a one.”\n“Excellent! So the likelihood of our data is higher for the fair die than for my loaded die, which is what led you to prefer the fair die model. Specifically you can calculate the likelihood of the data under your fair die model as \\(\\left(\\frac{1}{6}\\right)^5\\), and I can calculate the likelihood of the data under my loaded die model as \\((0.02)^5\\), which is way lower. We can apply the thinking for this scenario analogously to regression! In regression, we are proposing models for the data just like we were in this dice scenario. It’s just that we’re using a different probability model. Specifically, we assume that the outcomes come from a normal distribution and that the mean of the distribution is some linear combination of covariates. Knowing the covariate information for everyone in our data is just like knowing the sequence of die rolls, and knowing the density function for the normal distribution is just like knowing the probabilities for the six sides of the die!”\nThe last bit of this would vary depending on the student’s understanding of the assumptions and setup of linear regression, but I like this line of explanation more than my original one for two teaching strategies that it employs.\n Setting the student up to be contradicted. Arguably lessons are more memorable for students when they are fairly confident in an answer but end up being told otherwise. Here I wanted to make them think intuitively about a common situation but bring to light their unconscious assumptions by bringing in unusual but plausible assumptions of my own. One of the main points of the explanation, the key dependence of likelihood on the model being proposed, hinges on this awareness of the connection between having a model in mind and being able to calculate probabilities. So making this memorable is key. Framing the unfamiliar in the familiar. Abstract concepts are often so because students are not accustomed to the language that we instructors are using to describe them. However, by framing the concept in a familiar context, students can more easily make the leap. It’s also a nice way to explain something twice without just repeating yourself twice. Essentially, analogies are the way to go when explaining something complex.  ","date":1454987623,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1454987623,"objectID":"088725b4a5fa788753c020622246f597","permalink":"/post/what-is-likelihood/","publishdate":"2016-02-09T03:13:43Z","relpermalink":"/post/what-is-likelihood/","section":"post","summary":"I am currently a TA for an introductory biostatistics sequence at JHSPH where we teach students about the essentials of regression analysis. A great question that came up at office hours last week was, “What is likelihood?","tags":["likelihood"],"title":"What is likelihood?","type":"post"},{"authors":null,"categories":["Education"],"content":"As per a friend’s suggestion, I watched Conrad Wolfram’s TED talk on reforming mathematics education. He advocates increased use of computers in the classroom and, in particular, champions the idea of teaching math via programming. There were a lot of ideas both in and missing from his talk that I found interesting to think about.\nMathematics can be taught via programming Mr. Wolfram points to the procedural and algorithmic nature of mathematics to say that a fuller understanding of mathematics can be achieved by having students write programs that implement concepts. I completely agree that if you truly understand a concept, you can program it, but it’s worthwhile trying to consider how this might actually play out in middle and high school curricula.\nIt is useful to think about why this was such an interesting idea to many people in the first place. (It did get a TED talk after all.) The main point is that programming is not something that most people have any real idea of until they try it. I really didn’t understand what programming was about until I took an introductory class in college. I ended up loving it because I enjoyed the thorough and organized type of thinking that it encouraged, and I saw how useful it could be. People who are programmers or use programming regularly, like Mr. Wolfram, are very much convinced of its utility and are naturally excited about introducing this type of thinking earlier on in the education process. There are a few points that came to mind when weighing the pros and cons of a programming-oriented mathematics education:\nProgramming can be a great way to reinforce concepts I remember learning how to solve the tower of Hanoi puzzle in the children’s section of museums and in puzzle games long before I saw it again in my freshman year programming class. If placed before me, I probably would have been able to go through the motions from rough recollections or intuition, but the task at hand was to write a program that would solve an arbitrary tower of Hanoi puzzle (i.e. with any number of discs). Suddenly rough intuition needed to be transformed into a precise set of instructions, and it was the fact that I was seeing a familiar problem recast in a more general way that made the exercise relevant and memorable.\nWith standard pre-college mathematics education, a similar strategy could be adopted. As a specific example, solving systems of two equations with two unknowns is a standard algebra topic that could benefit from programming-type exercises. After students learn the technique of solving these problems, they hone their skills on practice problems and ideally are able to develop intuition on how to solve them by sight. Making this intuition more precise is where a programming exercise could come in. After students gain facility with solving the problems, we have them make their understanding more rigorous by asking them to write a very specific set of instructions to solve any such system of equations. Suddenly they must think about how exactly they choose the scaling numbers for the equations and how they choose to add or subtract the two equations.\nProgramming is probably not the best way to introduce a topic Just because an activity truly assesses understanding doesn’t mean that it automatically lays the foundation for a lesson plan. If all of my algebra lessons had been formatted as a series of instructions in the way that a programming perspective would emphasize, I know I would have enjoyed my math class a lot less. And I definitely would not have been ready to write a program to solve arbitrary systems of equations the day the topic was introduced. When material is presented as formulaic, as a procedure, students are compelled to simply memorize the steps involved in solving problems. The intuition is removed, and the concepts don’t stick past the next test. I still think that teachers should strive to motivate the material as much as possible—whether that be through telling a story or getting students to see how useful the concept can be. This is important in both a classroom instructional setting and in a homework design setting.\nSome features of programming can be useful for structuring material One of the main reasons why people write code is to organize groups of related procedures and perform them in a consistent way in many different occasions. I can see this being useful in a geometry classroom, for example. Students could organize their knowledge of geometrical formulas by having classes containing functions used for computing perimeters, areas, and volumes. Also, the process of writing the actual functions is an exercise in helping students remember precisely what quantities are needed to calculate others. The way that students would end up using these functions is another source of excitement for people who code regularly. Students would use their library of geometry functions to solve more involved problems by creating an organized sequence of function calls to solve each step of the problem in sequence. Programmers love being able to clearly see the overall flow of a complex procedure as a sequence of smaller tasks. Essentially, the function-oriented nature of programming enables students to put knowledge units into a documented story-like framework, which hopefully would encourage big picture understanding.\nTechnology can enrich education, not \u0026ldquo;dumb it down\u0026rdquo; While Mr. Wolfram’s big idea was to use programming to teach math, I think the main subtheme of his talk was the increased incorporation of technology in math classrooms. Not \u0026ldquo;technology for technology’s sake\u0026rdquo; in terms of gadgets like Smartboards and iPads, but thoughtfully constructed, computer-oriented lesson-plans, assignments, and activities. Technology, when appropriately used, can enrich education, and I think that the main avenue for this is through simulation and exploration.\nSimulation can be an amazing activity for getting students to think about real-world applications of what they are learning. For example, a lot of the simulations provided on this site have to do with the very practical task of learning about economics and could quite feasibly fit into an algebra curriculum. Just covered equations of lines? Well how about reinforcing those concepts and showing their utility by exploring linear trends that pop up in economics? If it is not common already, simulations should be more seriously considered by educators as add-on exercises to enhance a curriculum. I find simulations appealing as a teaching tool because they allow students to quickly try lots of things, allowing them to see a wide variety of phenomena in a short span of time. Most importantly, this allows them to discuss and write much more just because they have observed so much that they can comment on. Talking, and even more so writing due to its slower and more structured nature, is a great way to assess the extent to which students understand a topic, and if high quality writing is emphasized, it can really get students to internalize ideas. So in short, technology should be used as a means of facilitating a high volume of exploration so as to facilitate more writing and discussion.\nSummary Programming can definitely enrich math education by helping students organize concepts and reinforce their understanding of the material. However, we still have to make sure that we motivate material and put it into a memorable context for students. Regarding the broader goal of increasing the presence of technology in education, expensive and ineffective approaches should be abandoned for activities that can be performed on computers and equipment already available in schools. Activities such as simulations can markedly deepen investigation of a topic and are easily performed with the materials available in most classrooms. Amending curricula to incorporate these ideas and activities might involve some time investment, but it could definitely improve the quality of student learning.\n","date":1426871026,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1426871026,"objectID":"002eddf930c66c6f61df551eec0e40bb","permalink":"/post/teaching-real-math-with-computers/","publishdate":"2015-03-20T17:03:46Z","relpermalink":"/post/teaching-real-math-with-computers/","section":"post","summary":"As per a friend’s suggestion, I watched Conrad Wolfram’s TED talk on reforming mathematics education. He advocates increased use of computers in the classroom and, in particular, champions the idea of teaching math via programming.","tags":[],"title":"Teaching real math with computers","type":"post"},{"authors":null,"categories":["Education"],"content":"Part of my educational duties this past semester was as a teaching assistant for an undergraduate introductory biostatistics course. We went over the usual topics—calculating probabilities from tables, test statistics, hypothesis testing, linear and logistic regression—and I felt that the curriculum made a great effort to contextualize the material by organizing the content into goal-oriented modules. For example, linear regression was introduced as a tool for the specific goal of explaining college students’ GPA based on alcohol consumption-related characteristics. Whether or not the dataset that we gave to the students was the best source of information for investigating this relationship, I felt that there were two pedagogical ideas behind this module that were well implemented. First, the relationship being explored (I would guess) is one that piques the interest of a large percentage of the students. A lot of college students drink and almost everyone knows someone who drinks. It was great that the theme for this module was self-motivating. Second, the structure of the main assignment for the module forced students to write in a substantive way. The students had to come up with their own linear regression models to explore the relationship between GPA and drinking-related characteristics, and they were asked to write a report of their process, findings, and model interpretations—all things that are essential to understand when reading, writing, and discussing research findings.\nI want to focus on these two teaching ideas for a bit and give my perspective on what we might need to do to adapt education for the future. Current events have precipitated a lot of intense and particularly emotion-backed discussion about racial injustice and inequality in general. From discussions with people much more knowledgeable and well-spoken than I am, I would have to say that perhaps the most essential asset that a lot of people don’t get from their education is an ability to think critically and argue rationally. These are hard things to do, and I think that school is the place to get people to practice.\nAs I’ve explained in previous posts, case-based learning is a great way to motivate the concepts being taught in class so that students can see their practical uses and therefore remember the ideas for the long-term. In creating examples and giving context to classroom content, we need to think hard about what those examples will be. Perhaps the best way to really engage students and get them to care about what they’re learning is to use the most current issues possible. For example, I think that someone who decided to create a case-based statistics course for the current generation might have a lot of success using both news and research articles about social inequality. I think the key is to relate what is being presented in the news to what research has actually been performed and get students to closely examine the relationship between these sources of information.\nRational argumentation is another skill that is lacking in our generation. Very often we tend to talk to and become friends with like-minded people, and we are not as easily forced to question our views and see fallacies in the bases for our opinions. Forcing conversation in an educational setting is a great way to break that comfort zone because there is such a diversity of opinion. I love that the statistics course that I taught for emphasized writing because it forced students to at least put some words behind what they were learning. Just having those words is several steps above rote calculation and memorization in terms of really embedding meaning and understanding. But the way to truly make the most of those words is to construct them carefully to tell a story, to make a point. Words as a list of facts do little to reinforce understanding. To this end, I think that educators should elevate the role of writing to the level of speech and debate. I see this having great potential in mathematics and statistics classrooms. Throughout a statistics course we teach students about tools and the assumptions behind them that are used to draw conclusions from data. To truly assess their understanding of statistical concepts and to put these concepts in a meaningful context, perhaps one of the key assignments or activities of the course would be to read scientific papers and have a debate. In this way, we can prepare students for the types of discussions that they’ll have throughout their life by helping them recognize reasoning flaws in the arguments of others as well as their own.\n","date":1419442890,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1419442890,"objectID":"ccbd171da66a6cfa4b2b32cdfc872bb1","permalink":"/post/adapting-education-style/","publishdate":"2014-12-24T17:41:30Z","relpermalink":"/post/adapting-education-style/","section":"post","summary":"Part of my educational duties this past semester was as a teaching assistant for an undergraduate introductory biostatistics course. We went over the usual topics—calculating probabilities from tables, test statistics, hypothesis testing, linear and logistic regression—and I felt that the curriculum made a great effort to contextualize the material by organizing the content into goal-oriented modules.","tags":[],"title":"Adapting education style to improve relevance and practical skills","type":"post"},{"authors":null,"categories":["Education"],"content":"Apathy is the cancer of today’s classroom. Once it plants its nasty little head in a student’s mind, it can be one tough beast to eradicate. Complaints like “I don’t care about this” and “When would I even use this?” are frighteningly common in higher education and indicate a malady far worse than boredom: time wasted. Not only are students wasting time being in class, cranking out hours of work to learn material that won’t be retained or appreciated, but teachers are also wasting their time preparing mechanical, need-agnostic material that will ultimately make no impact on their students’ interaction with the world.\nIn one way or another, the point of education is to learn how to live in this world—whether to learn specific skills for a job that will pay for our livelihoods or to learn ideas that shape how we react to the people, laws, and situations around us. And one of the most important ways in which education applies in real life is in being able to recognize when and how different concepts pertain to the situation at hand. Too often in classrooms are we given the punch line before the build-up. As education blogger Dan Meyer puts it, teachers are too excited to present the concept without spending an adequate enough time motivating the question behind the concept. And the result is a lack of internalization of ideas, a failure to understand why the material being taught is relevant.\nAs I explained in my last post, the Interactive Mathematics Program (IMP) puts in an admirable effort to motivate the need for mathematical concepts. It presents a tough multi-faceted problem at the beginning of each unit and develops the need for various math topics as it pertains to this overarching problem. I felt that one of the most memorable and instructive units was one in which we attempted to solve the unit problem on the very first day without any formal tools whatsoever. The best part (though frustrating at the time for me) was that this was really hard. There is definitely something that can be said for having students try to do things inefficiently to learn the merits of the concepts that teachers are so excited to get to. Or in Dan’s words, there is definitely something that can be said for \u0026ldquo;being less helpful\u0026rdquo;—not jumping straight to teaching students about power tools but momentarily convincing them that the logs on their desks can only be cut with butter knives.\nIntroductory statistics courses can definitely benefit from a more problem-oriented and \u0026ldquo;less helpful\u0026rdquo; mentality. For one, I think there is a deluge of formulae for students to learn, and most of the time, the reasons for using them were never really developed or lost in the memorization process. The topic of confidence intervals serves as a great example here. We drill into students’ minds that the formula for a confidence interval is an estimate plus or minus some multiple of the standard error. And we can tell them that the interval gives us a range of possible values for the true population mean. But why are these possible values good ones? Why do we even have to give a confidence interval? Why isn’t it enough to just say that the mean is around 5, say? When do I ever see confidence intervals in real life?\nA better way to teach this idea than to present a formula and give a two-minute interpretation is to make students see how the idea of a confidence interval is one they are exposed to frequently but in disguise. One way to do this is to present the students with common types of advertisement tricks.\nImage credit\nImage credit\nThe statistics presented on these ads are instinctively somewhat convincing. Hey a 4% reduction in cholesterol is pretty good. Wow, 90% of doctors recommend this product! These are definitely worth buying! But we can also think about what numbers on these ads would make us less convinced of the products' worth. A cholesterol reduction of 0 to 1% would definitely not make me want to buy Cheerios. And I would be much less impressed by this Colgate toothpaste if 50% or less of doctors recommended it.\nNow we give the students data—several sets of data that support or fail to substantiate the Cheerios claim to varying degrees—and ask them to make their own conclusions based on this data. I would expect many of them to take the average lowering of cholesterol as a summary measure; some might look at the median or mode; some might look at the percentage of people who had their cholesterol lowered by some minimal percentage level. No matter how they choose to look at the data the key idea is that the students see how their chosen summary measure(s) vary from dataset to dataset. Sometimes the claims in the ad are supported, and other times they are definitely not. It’s just that companies often only report their summary measures and not how much that measure would vary had they tested their product on different groups of individuals.\nAfter this point, I think that students would be a little more ready to learn about confidence intervals because they have seen how advertisements, something they encounter all the time, can use (or really conceal) them in misleading ways. And no one likes being duped.\nThis is by far not the best way of motivating confidence intervals, but it is a lot more than can be said for the majority of introductory statistics classes. Just taking a little bit more time to think about the everyday applications of statistics can go a long way in making lessons less formulaic and more engaging, and this is something that the statistics community should strive for.\n","date":1412552545,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1412552545,"objectID":"35e08310baf831c85eb4aa299106ce8d","permalink":"/post/motivating-the-question/","publishdate":"2014-10-05T23:42:25Z","relpermalink":"/post/motivating-the-question/","section":"post","summary":"Apathy is the cancer of today’s classroom. Once it plants its nasty little head in a student’s mind, it can be one tough beast to eradicate. Complaints like “I don’t care about this” and “When would I even use this?","tags":[],"title":"Motivating the question","type":"post"},{"authors":null,"categories":["Education"],"content":"My pre-college mathematics education was probably different from most others. Instead of adopting the standard approach of teaching Algebra I and II, Geometry, and Trigonometry, my school district took up the Interactive Mathematics Program (IMP), a problem-centric approach to learning the essential material from these subjects. The program was split into 4 courses, each the equivalent of a middle school year or high school semester, and each course was split into approximately 5 units. Each unit introduced a mathematically-oriented story, which gave rise to a guiding question that we sought to be able to answer over the course of the unit. For example, in the second year unit “Cookies”, we are introduced to the Woo’s, a family of bakers who wish to optimize their cookie baking choices to maximize their profits. The challenge though is that they have several constraints on their available ingredients and sales capabilities. Over the course of the unit, we learned several ideas related to systems of equations and inequalities that helped us answer this question.\nAlthough I didn’t appreciate it at the time, IMP was attempting to do something that is rarely seen in education these days but is extremely important: motivating the concepts. At the beginning of the “Cookies” unit, I remember that class immediately started off with an attempt to solve the unit problem—no instruction on the best mathematical techniques, just a lot of pain, backtracking, and guesswork. I remember being overwhelmed just after organizing the information provided on the family’s ingredient and sales constraints. And guessing potential solutions was memorably laborious: does it work to make 100 chocolate chip and 170 oatmeal raisin? Rats! That violates constraint 4! What about 150 oatmeal raisin? Nice! That’s allowed! Aw shoot, that makes less money than my 150 chocolate chip, 130 oatmeal raisin combo! Fifty minutes of this and I was pulling my hair out. There must be an easier way to do this! And there was! Which we came to learn over the course of the next month or so that we spent on the unit. This particular unit sticks out in my memory primarily because it did such a good job at motivating the need for the main mathematical tools that we were learning during the unit. Had I taken a standard algebra course, I know I would not have truly internalized the utility of what we learned during that unit. For the most part, IMP did a decent job at contextualizing math concepts, and I think that this is a quality that is lacking in higher education in general. In designing curricula, we are not doing enough to motivate the content of the course, and I plan to explore this idea much further in a future post relating to introductory statistics curricula.\nAnother aspect of IMP that is particularly well suited to higher education is its emphasis on summarizing and organizing knowledge gained over the course of a unit. At the end of every unit, students are required to create a portfolio of reflections, class notes, and assignments that were instrumental in their comprehension of the main topics. So for example, in my “Cookies” portfolio, I included the first day of class activity that had us take a stab at the unit problem. I also included my class notes on feasible regions and solving systems of linear inequalities. I also wrote a cover letter summarizing the goals of the unit, what I learned from the assignments that I included in the portfolio, and my impressions of how these concepts were useful in everyday life. Portfolio time was always a laborious and dreadful experience for me during middle and high school, but I recognize now how useful a practice it is for being serious about retaining knowledge. As a graduate student, I have been exposed to several fundamental concepts time and time again in different courses, and I have found myself consciously wishing that I had put together portfolios for many of the classes that I took during college. Not only is the act of putting together a portfolio an invaluable synthesis activity, but the portfolio also serves as a one-of-a-kind reference manual. Because it is a collection of notes, homeworks, etc. created, curated, and edited by you, it can be infinitely more readable then a textbook. If you want it to, it can contain all of the steps in the proofs of key theorems, all of the exhaustive explanation that you worked through on your own, all of the fine details that finally made a concept click. A portfolio is a mini-textbook written in the language of your mind and can thus potentially serve as a reference for a very long time. It is a wonderful idea for IMP to introduce this concept to younger students, and I feel that it would be very useful for many high school and college classes to adopt.\nSo essentially the Interactive Mathematics Program, though by no means flawless, takes steps beyond traditional education practices that definitely have merit. Its case-based structuring of learning and its emphasis on creating concept portfolios can really impact the depth of learning and are ideas that nearly all higher education courses can benefit from.\n","date":1411441065,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1411441065,"objectID":"e30f56fc8cc620967a754191cdba09f1","permalink":"/post/imp-perspectives/","publishdate":"2014-09-23T02:57:45Z","relpermalink":"/post/imp-perspectives/","section":"post","summary":"My pre-college mathematics education was probably different from most others. Instead of adopting the standard approach of teaching Algebra I and II, Geometry, and Trigonometry, my school district took up the Interactive Mathematics Program (IMP), a problem-centric approach to learning the essential material from these subjects.","tags":[],"title":"Perspectives on the Interactive Mathematics Program","type":"post"},{"authors":null,"categories":["Opinion"],"content":"  I’ve been planning for some time to start this blog mainly as a way to give intuition to people untrained in statistics about what statisticians do and why. For the last few weeks, I’ve been gathering ideas, and I’ve found that in the process I’ve had to be a lot more reflective about fundamental aims in statistics that I had almost started taking for granted. So as a brief aside, I highly encourage people to blog! Everyone cares about something, and blogging is a great way to (1) make sure you know your stuff, (2) learn stuff, and (3) share stuff. As a PhD student (1) and (2) are constantly among my worries, and I care about (3) because I love teaching and talking about ideas that are interesting to me.\nSo let’s get to the interesting stuff (I hope)! I love statistics, but I know that most people do not feel the same way that I do. I know I can’t inspire a love of the field for everyone who reads this, but I do want to motivate statistics and offer some opinions as to why statistics tends to elicit grimaces from non-practitioners.\nAs I was planning to start this blog, I asked my friends about their opinions about statistics, and it was pointed out that a particularly unsatisfying part of learning statistics is that students tend to get the feeling that they aren’t certain about anything. I won’t try to deny this because it’s completely true. But when are we ever 100% certain about anything interesting? See if you can come up with an example of something that you know with absolute certainty and that actually raises your eyebrows. I’m coming up with “I will type the word ‘statistics’ at least one more time in this post” and “I will eat a brownie in the next week.” These are completely inevitable and therefore (to me) completely uninteresting statements. Even inevitabilities can have degrees of uncertainty when we pose them in a different way:\n “A new president will be elected next term.” But who will win the election? “March Madness will come to an end soon.” But what team will come out on top? “Another research paper in statistical genomics will be published in the next month.” But will it be any good? “The government will spend money on scientific research next year.” But how much?  The point is that although we are certain about the occurrence or non-occurrences of many events, we tend to be more interested in the details surrounding these events, and these details are what give rise to uncertainty. Statistics cannot change this inherent uncertainty in the world but it can give us a tool to make more informed predictions. For example, if I were trying to find the best place to buy gas in my neighborhood, I could look at the daily history of prices per gallon at all of the local stations and do a statistical analysis to determine which station tends to be the cheapest. Statistics will not allow me to be certain of the price per gallon at these stations tomorrow. The only way I could know this with certainty is if I had infinite and perfect knowledge of how gas prices are determined each day. This would entail knowing everything about the workings of the economy and the mindsets of CEOs of gas companies among many other things. But I don’t have this infinite knowledge. I have to work with the limited data available to me, and statistics is a structured framework for doing so.\nThe structure in statistics is in large part governed by models. Model specification is a core part of statistics, and I contend that it is also what makes our field so scary. Models can be useful because they allow us to propose reasons for why we observe what we do in a concrete, structured, and reproducible way. I’ll illustrate this with an example.\nSuppose you work at a company that does a weekly lottery for a gift certificate to a local restaurant. Each week, a computer randomly selects one of the 500 total employees to receive the prize so that each of the 500 employees has an equal chance of being picked (1 in 500). For many, intuition says, “I should have a higher chance of being selected as the weeks go by.” This is not a correct statement, but it is an easy mistake to make if you are not thinking about the mathematics behind the statement. This is why models can be useful. They can help us translate statements like these into mathematical expressions that precisely quantify the probabilities we are trying to calculate. This lottery is exactly an example of what statisticians would call a “binomial experiment.” It is a model that allows us to calculate the probability of different numbers of successes in a series of independent trials and allows us to supply the (fixed) probability of success, the number of trials, and the number of successes. So in this situation, the fixed probability of success is 1/500, the number of trials is the number of weeks the lottery has been taking place, and the number of successes of interest is zero because we are interested in the probability of never being picked in a given number of weeks.\nThe statement “I should have a higher chance of being selected as the weeks go by” translates to the question “What is the probability that I am chosen in week X?” and is a question that is easily answerable by our model of the lottery. Because our model requires a fixed probability of success (1/500), the probability of being chosen in week X is 1/500 regardless of what week it is. More intuitively, why is this so? Our question is one about the characteristics of the lottery, which is a fixed procedure. We can’t change the fact that there are 500 employees, and we can’t change the computer that is picking between the 500 employees equally.\nHowever, by keeping in mind that our model of the lottery allows us to calculate probabilities of numbers of successes, we should instead ask, “What is the probability that I am chosen exactly zero times in X weeks?” which translates to the intuition “It should be increasing unlikely for me to never be picked in all the weeks the lottery has been going on.” This question concerns the outcome of the lottery, which is uncertain, whereas we had previously been inquiring about a characteristic of the lottery design, which was quite certain. Our model easily answers this question, and the probabilities as a function of the number of weeks X are shown in the graph on the left below. The probabilities for the complementary question “What is the probability that I am chosen at least once in X weeks?” are shown below on the right. (The R code for making the plots is also shown below.)\np \u0026lt;- 1/500 weeks \u0026lt;- 1:500 prob_notchosen \u0026lt;- (1-p)^weeks prob_chosen \u0026lt;- 1-prob_notchosen par(mfrow = c(1,2), mar = c(4,4,2,1), bty = \u0026quot;l\u0026quot;) plot(weeks, prob_notchosen, xlab = \u0026quot;Week\u0026quot;, ylab = \u0026quot;Probability\u0026quot;, main = \u0026quot;Never being chosen\u0026quot;, type = \u0026quot;l\u0026quot;, ylim = c(0,1)) plot(weeks, prob_chosen, xlab = \u0026quot;Week\u0026quot;, ylab = \u0026quot;Probability\u0026quot;, main = \u0026quot;Chosen at least once\u0026quot;, type = \u0026quot;l\u0026quot;, ylim = c(0,1)) We can see that as weeks go by, the probability of never once being chosen declines, and the complementary probability of being chosen at least once increases. So the initial intuition “I should have a higher chance of being selected as the weeks go by” is wrong because we did not carefully form our question. What we really wanted to express is the increasing probability of being picked at least once in a larger and larger time frame. And this was a lot easier to express once we had cast the lottery as a binomial model and determined the types of questions that our model was suited to answer.\nIn this example of the lottery, the binomial model was an exact description of the situation. But very rarely in real life are the situations we want to study so clear cut. Most of the time interesting systems are complex, and statisticians have to make many simplifying assumptions in order to even begin the modeling process. The oft-quoted line from George Box illustrates this situation “Essentially, all models are wrong, but some are useful.” The “some are useful” part of Box’s quote is what can make statistics so scary. How can we know whether our models are really useful? In other words, how can we know whether our models still closely approximate reality? We tend to fall in love with our models because we invest so much time in them, and this is partly why our collaborators in non-statistical fields find our process mystifying. We often adopt models because they seem to fit reality well enough and mostly because they are convenient. But in order to make meaningful discoveries, we can’t just guess and assume that our reasoning alone justifies the models we create. We either need to do a better job at incorporating the extensive knowledge of field-specific experts into our models or restrain the urge to take the modeling approach altogether.\n","date":1395888777,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1395888777,"objectID":"d528826013687b9b084ae37db2381c58","permalink":"/post/why-statistics-can-be-scary/","publishdate":"2014-03-27T02:52:57Z","relpermalink":"/post/why-statistics-can-be-scary/","section":"post","summary":"I’ve been planning for some time to start this blog mainly as a way to give intuition to people untrained in statistics about what statisticians do and why. For the last few weeks, I’ve been gathering ideas, and I’ve found that in the process I’ve had to be a lot more reflective about fundamental aims in statistics that I had almost started taking for granted.","tags":[],"title":"Why statistics can be scary","type":"post"}]